{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "\n",
    "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but\n",
    "it’s the point of this exercise). Use He initialization and the Swish activation\n",
    "function.\n",
    "\n",
    "b. Using Nadam optimization and early stopping, train the network on the\n",
    "CIFAR10 dataset. You can load it with tf.keras.datasets.cifar10.load_\n",
    "data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000\n",
    "for training, 10,000 for testing) with 10 classes, so you’ll need a softmax\n",
    "output layer with 10 neurons. Remember to search for the right learning rate\n",
    "each time you change the model’s architecture or hyperparameters.\n",
    "\n",
    "c. Now try adding batch normalization and compare the learning curves: is it\n",
    "converging faster than before? Does it produce a better model? How does it\n",
    "affect training speed?\n",
    "\n",
    "d. Try replacing batch normalization with SELU, and make the necessary adjust‐\n",
    "ments to ensure the network self-normalizes (i.e., standardize the input fea‐\n",
    "tures, use LeCun normal initialization, make sure the DNN contains only a\n",
    "sequence of dense layers, etc.).\n",
    "\n",
    "e. Try regularizing the model with alpha dropout. Then, without retraining your\n",
    "model, see if you can achieve better accuracy using MC dropout.\n",
    "\n",
    "f. Retrain your model using 1cycle scheduling and see if it improves training\n",
    "speed and model accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Building the DNN with 20 hidden layers of 100 neurons each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Suat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer HeNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, \n",
    "                                 activation=keras.layers.Activation('swish'), \n",
    "                                 kernel_initializer=initializer))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Training the network with Nadam optimization and early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 21s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 19s 9ms/step - loss: 2.0216 - accuracy: 0.2217 - val_loss: 1.9425 - val_accuracy: 0.2594\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.8732 - accuracy: 0.2957 - val_loss: 1.8134 - val_accuracy: 0.3114\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.8039 - accuracy: 0.3293 - val_loss: 1.7727 - val_accuracy: 0.3450\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.7530 - accuracy: 0.3543 - val_loss: 1.7332 - val_accuracy: 0.3694\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.7141 - accuracy: 0.3763 - val_loss: 1.7161 - val_accuracy: 0.3710\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.6892 - accuracy: 0.3896 - val_loss: 1.6701 - val_accuracy: 0.3969\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 11s 8ms/step - loss: 1.6521 - accuracy: 0.4038 - val_loss: 1.6793 - val_accuracy: 0.3996\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.6338 - accuracy: 0.4121 - val_loss: 1.6823 - val_accuracy: 0.3975\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.6096 - accuracy: 0.4234 - val_loss: 1.6388 - val_accuracy: 0.4172\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.5951 - accuracy: 0.4317 - val_loss: 1.6343 - val_accuracy: 0.4126\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.5724 - accuracy: 0.4365 - val_loss: 1.6250 - val_accuracy: 0.4301\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.5534 - accuracy: 0.4444 - val_loss: 1.6141 - val_accuracy: 0.4275\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.5440 - accuracy: 0.4493 - val_loss: 1.6362 - val_accuracy: 0.4264\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.5323 - accuracy: 0.4520 - val_loss: 1.6307 - val_accuracy: 0.4200\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.5215 - accuracy: 0.4581 - val_loss: 1.6166 - val_accuracy: 0.4314\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.5094 - accuracy: 0.4636 - val_loss: 1.6270 - val_accuracy: 0.4210\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.4979 - accuracy: 0.4670 - val_loss: 1.6062 - val_accuracy: 0.4349\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.4873 - accuracy: 0.4718 - val_loss: 1.6092 - val_accuracy: 0.4338\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.4760 - accuracy: 0.4735 - val_loss: 1.6164 - val_accuracy: 0.4283\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.4632 - accuracy: 0.4782 - val_loss: 1.6308 - val_accuracy: 0.4282\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.4549 - accuracy: 0.4833 - val_loss: 1.6752 - val_accuracy: 0.4197\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.4462 - accuracy: 0.4859 - val_loss: 1.6079 - val_accuracy: 0.4293\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.4329 - accuracy: 0.4894 - val_loss: 1.6599 - val_accuracy: 0.4246\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.4231 - accuracy: 0.4952 - val_loss: 1.6145 - val_accuracy: 0.4309\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.4157 - accuracy: 0.4975 - val_loss: 1.6461 - val_accuracy: 0.4331\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.4029 - accuracy: 0.5003 - val_loss: 1.6423 - val_accuracy: 0.4291\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.3922 - accuracy: 0.5039 - val_loss: 1.6373 - val_accuracy: 0.4349\n"
     ]
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Nadam(lr=1e-3), metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train_full, y_train_full, epochs=100, validation_split=0.2, callbacks=[early_stopping_cb])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Adding batch normalization to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 27s 13ms/step - loss: 1.9716 - accuracy: 0.2736 - val_loss: 2.0564 - val_accuracy: 0.2859\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.7620 - accuracy: 0.3653 - val_loss: 1.8347 - val_accuracy: 0.3569\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6748 - accuracy: 0.4020 - val_loss: 1.8056 - val_accuracy: 0.3785\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.6040 - accuracy: 0.4279 - val_loss: 1.7163 - val_accuracy: 0.3933\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.5571 - accuracy: 0.4445 - val_loss: 1.6618 - val_accuracy: 0.4163\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.5110 - accuracy: 0.4629 - val_loss: 1.7899 - val_accuracy: 0.3813\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 1.4652 - accuracy: 0.4792 - val_loss: 1.7743 - val_accuracy: 0.3860\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.4241 - accuracy: 0.4922 - val_loss: 1.6127 - val_accuracy: 0.4350\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.3855 - accuracy: 0.5098 - val_loss: 1.4841 - val_accuracy: 0.4815\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.3518 - accuracy: 0.5228 - val_loss: 1.5306 - val_accuracy: 0.4714\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3274 - accuracy: 0.5293 - val_loss: 1.6035 - val_accuracy: 0.4367\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2909 - accuracy: 0.5439 - val_loss: 1.6792 - val_accuracy: 0.4386\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2680 - accuracy: 0.5525 - val_loss: 1.5159 - val_accuracy: 0.4739\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.2390 - accuracy: 0.5635 - val_loss: 1.5497 - val_accuracy: 0.4617\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 1.2211 - accuracy: 0.5672 - val_loss: 1.4807 - val_accuracy: 0.4817\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1913 - accuracy: 0.5805 - val_loss: 1.4727 - val_accuracy: 0.4905\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1709 - accuracy: 0.5867 - val_loss: 1.5552 - val_accuracy: 0.4662\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1549 - accuracy: 0.5956 - val_loss: 1.5551 - val_accuracy: 0.4541\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.1409 - accuracy: 0.5970 - val_loss: 1.5620 - val_accuracy: 0.4618\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1147 - accuracy: 0.6049 - val_loss: 1.5211 - val_accuracy: 0.4838\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0937 - accuracy: 0.6123 - val_loss: 1.5945 - val_accuracy: 0.4621\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0788 - accuracy: 0.6208 - val_loss: 1.5812 - val_accuracy: 0.4729\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0592 - accuracy: 0.6256 - val_loss: 1.6979 - val_accuracy: 0.4484\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0470 - accuracy: 0.6315 - val_loss: 1.6651 - val_accuracy: 0.4549\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.0391 - accuracy: 0.6338 - val_loss: 1.5772 - val_accuracy: 0.4789\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 1.0152 - accuracy: 0.6418 - val_loss: 1.6764 - val_accuracy: 0.4584\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, use_bias=False))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('swish'))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Nadam(lr=1e-3), metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train_full, y_train_full, epochs=100, validation_split=0.2, callbacks=[early_stopping_cb])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Replacing batch normalization with SELU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 21s 11ms/step - loss: 2.1147 - accuracy: 0.1889 - val_loss: 1.9534 - val_accuracy: 0.2606\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.9518 - accuracy: 0.2628 - val_loss: 1.9829 - val_accuracy: 0.2902\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.9128 - accuracy: 0.2810 - val_loss: 1.8798 - val_accuracy: 0.3027\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.8576 - accuracy: 0.3065 - val_loss: 1.8424 - val_accuracy: 0.3208\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.8347 - accuracy: 0.3182 - val_loss: 1.8773 - val_accuracy: 0.3111\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.8118 - accuracy: 0.3258 - val_loss: 1.8038 - val_accuracy: 0.3393\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.8097 - accuracy: 0.3318 - val_loss: 1.7909 - val_accuracy: 0.3274\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.7630 - accuracy: 0.3552 - val_loss: 1.8394 - val_accuracy: 0.3301\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.7738 - accuracy: 0.3536 - val_loss: 1.7656 - val_accuracy: 0.3632\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.8290 - accuracy: 0.3251 - val_loss: 2.0451 - val_accuracy: 0.1934\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.9550 - accuracy: 0.2508 - val_loss: 1.8658 - val_accuracy: 0.2788\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.8494 - accuracy: 0.2898 - val_loss: 1.8537 - val_accuracy: 0.3031\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.8267 - accuracy: 0.3076 - val_loss: 1.8495 - val_accuracy: 0.3112\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.8091 - accuracy: 0.3256 - val_loss: 1.8176 - val_accuracy: 0.3262\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.7858 - accuracy: 0.3364 - val_loss: 1.7685 - val_accuracy: 0.3484\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.7785 - accuracy: 0.3397 - val_loss: 1.7856 - val_accuracy: 0.3480\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.8202 - accuracy: 0.3229 - val_loss: 1.9236 - val_accuracy: 0.2918\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.8038 - accuracy: 0.3289 - val_loss: 1.8395 - val_accuracy: 0.3104\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.7801 - accuracy: 0.3386 - val_loss: 1.7474 - val_accuracy: 0.3611\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 2.7417 - accuracy: 0.2822 - val_loss: 1.9051 - val_accuracy: 0.2761\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.8452 - accuracy: 0.2961 - val_loss: 1.8264 - val_accuracy: 0.3189\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.7907 - accuracy: 0.3256 - val_loss: 1.8141 - val_accuracy: 0.3179\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.7577 - accuracy: 0.3452 - val_loss: 1.7743 - val_accuracy: 0.3448\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.7495 - accuracy: 0.3493 - val_loss: 1.7911 - val_accuracy: 0.3285\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.7539 - accuracy: 0.3458 - val_loss: 1.8160 - val_accuracy: 0.3420\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.7582 - accuracy: 0.3479 - val_loss: 1.9033 - val_accuracy: 0.3073\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.8038 - accuracy: 0.3240 - val_loss: 1.8249 - val_accuracy: 0.3103\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.7188 - accuracy: 0.3578 - val_loss: 1.7397 - val_accuracy: 0.3630\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.6827 - accuracy: 0.3756 - val_loss: 1.7851 - val_accuracy: 0.3647\n",
      "Epoch 30/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.6949 - accuracy: 0.3730 - val_loss: 1.7253 - val_accuracy: 0.3566\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.6564 - accuracy: 0.3892 - val_loss: 1.7143 - val_accuracy: 0.3796\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.6391 - accuracy: 0.4002 - val_loss: 1.7169 - val_accuracy: 0.3698\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.6259 - accuracy: 0.4046 - val_loss: 1.6739 - val_accuracy: 0.4024\n",
      "Epoch 34/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.7879 - accuracy: 0.3894 - val_loss: 1.7155 - val_accuracy: 0.3707\n",
      "Epoch 35/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.6333 - accuracy: 0.4008 - val_loss: 1.6970 - val_accuracy: 0.3855\n",
      "Epoch 36/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.6163 - accuracy: 0.4112 - val_loss: 1.6545 - val_accuracy: 0.4031\n",
      "Epoch 37/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.6029 - accuracy: 0.4170 - val_loss: 1.6692 - val_accuracy: 0.4044\n",
      "Epoch 38/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.5874 - accuracy: 0.4219 - val_loss: 1.7036 - val_accuracy: 0.3988\n",
      "Epoch 39/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.5811 - accuracy: 0.4254 - val_loss: 1.6794 - val_accuracy: 0.3971\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.5877 - accuracy: 0.4245 - val_loss: 1.6721 - val_accuracy: 0.4109\n",
      "Epoch 41/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.5766 - accuracy: 0.4289 - val_loss: 1.6511 - val_accuracy: 0.4033\n",
      "Epoch 42/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.5709 - accuracy: 0.4328 - val_loss: 1.6678 - val_accuracy: 0.4065\n",
      "Epoch 43/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.5674 - accuracy: 0.4342 - val_loss: 1.6516 - val_accuracy: 0.4098\n",
      "Epoch 44/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.5703 - accuracy: 0.4313 - val_loss: 1.6493 - val_accuracy: 0.4165\n",
      "Epoch 45/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.5475 - accuracy: 0.4429 - val_loss: 1.6804 - val_accuracy: 0.3962\n",
      "Epoch 46/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.5687 - accuracy: 0.4332 - val_loss: 1.6348 - val_accuracy: 0.4247\n",
      "Epoch 47/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.5540 - accuracy: 0.4391 - val_loss: 1.6420 - val_accuracy: 0.3975\n",
      "Epoch 48/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.5385 - accuracy: 0.4484 - val_loss: 1.6416 - val_accuracy: 0.4175\n",
      "Epoch 49/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.5634 - accuracy: 0.4336 - val_loss: 1.7128 - val_accuracy: 0.4037\n",
      "Epoch 50/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.5969 - accuracy: 0.4186 - val_loss: 1.6614 - val_accuracy: 0.4037\n",
      "Epoch 51/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.5330 - accuracy: 0.4435 - val_loss: 1.6741 - val_accuracy: 0.4112\n",
      "Epoch 52/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.5912 - accuracy: 0.4268 - val_loss: 1.6358 - val_accuracy: 0.4198\n",
      "Epoch 53/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.6885 - accuracy: 0.3891 - val_loss: 1.8414 - val_accuracy: 0.3372\n",
      "Epoch 54/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.6882 - accuracy: 0.3759 - val_loss: 1.7024 - val_accuracy: 0.3884\n",
      "Epoch 55/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.7038 - accuracy: 0.3758 - val_loss: 1.8636 - val_accuracy: 0.3174\n",
      "Epoch 56/100\n",
      "1250/1250 [==============================] - 13s 11ms/step - loss: 1.7871 - accuracy: 0.3385 - val_loss: 1.9550 - val_accuracy: 0.2641\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.initializers import lecun_normal\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation, Dropout\n",
    "\n",
    "def selu(x):\n",
    "    alpha = 1.67326\n",
    "    scale = 1.0507\n",
    "    return scale * K.elu(x, alpha)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(Dense(100, activation=selu, kernel_initializer=lecun_normal()))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Nadam(lr=1e-3), metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train_full, y_train_full, epochs=100, validation_split=0.2, callbacks=[early_stopping_cb])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. regularizing the model with alpha dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 20s 11ms/step - loss: 2.3703 - accuracy: 0.1175 - val_loss: 4.1109 - val_accuracy: 0.1696\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 2.1535 - accuracy: 0.1595 - val_loss: 10.8815 - val_accuracy: 0.1739\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 2.1115 - accuracy: 0.1663 - val_loss: 6.5563 - val_accuracy: 0.1711\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 2.0836 - accuracy: 0.1764 - val_loss: 9.1327 - val_accuracy: 0.1697\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 2.0640 - accuracy: 0.1826 - val_loss: 7.6539 - val_accuracy: 0.1666\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 13s 11ms/step - loss: 2.0536 - accuracy: 0.1777 - val_loss: 7.9135 - val_accuracy: 0.1751\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 2.0444 - accuracy: 0.1782 - val_loss: 4.8593 - val_accuracy: 0.1703\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 2.0416 - accuracy: 0.1784 - val_loss: 6.6309 - val_accuracy: 0.1662\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 2.0345 - accuracy: 0.1757 - val_loss: 4.5003 - val_accuracy: 0.1641\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 2.0312 - accuracy: 0.1788 - val_loss: 4.8407 - val_accuracy: 0.1646\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 2.0251 - accuracy: 0.1798 - val_loss: 4.3982 - val_accuracy: 0.1790\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=keras.layers.Activation('swish'), kernel_initializer=initializer))\n",
    "    model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Nadam(lr=1e-3), metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train_full, y_train_full, epochs=100, validation_split=0.2, callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 6ms/step\n",
      "79/79 [==============================] - 0s 6ms/step\n",
      "79/79 [==============================] - 0s 6ms/step\n",
      "79/79 [==============================] - 0s 6ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "Test accuracy: 0.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# make predictions with MC dropout\n",
    "y_probas = np.stack([model.predict(X_test, batch_size=128, verbose=1)\n",
    "                     for sample in range(100)])\n",
    "\n",
    "# compute mean and standard deviation of predictions\n",
    "y_mean = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)\n",
    "\n",
    "# evaluate accuracy\n",
    "accuracy = np.mean(np.equal(y_test, np.argmax(y_mean, axis=1)))\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Retraining model using 1cycle scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 16s 27ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.0000e+00\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 8s 25ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.0250\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 8s 25ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.0500\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 8s 26ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.0750\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 8s 26ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.1000\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 8s 25ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.1250\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 8s 25ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.1500\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 8s 26ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.1750\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 8s 25ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.2000\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 8s 25ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.2250\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 8s 26ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.2500\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 8s 26ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.2750\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 8s 26ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.3000\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 8s 26ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.3250\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 8s 27ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.3500\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 8s 25ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.3750\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 7s 24ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.4000\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 7s 24ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.4250\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 8s 25ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.4500\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 9s 28ms/step - loss: nan - accuracy: 0.0997 - val_loss: nan - val_accuracy: 0.1014 - lr: 0.4750\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def one_cycle_lr(epoch, lr):\n",
    "    max_lr = 0.05\n",
    "    end_percentage = 0.1\n",
    "    step_size = 5 * (len(X_train_full) // 128)\n",
    "    midpoint = step_size // 2\n",
    "    momentum = 0.95\n",
    "    \n",
    "    if epoch <= midpoint:\n",
    "        new_lr = ((max_lr / end_percentage) / 2) * epoch * end_percentage\n",
    "    else:\n",
    "        new_lr = ((max_lr / end_percentage) / 2) * (1 - (epoch - midpoint) * (1 - end_percentage) / (step_size - midpoint))\n",
    "        \n",
    "    model.optimizer.lr = new_lr\n",
    "    model.optimizer.beta_1 = momentum\n",
    "    return new_lr\n",
    "\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(one_cycle_lr)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Nadam(lr=1e-3, beta_1=0.95), metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train_full, y_train_full, epochs=20, validation_split=0.2, batch_size=128, callbacks=[lr_scheduler])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
